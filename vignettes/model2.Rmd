---
title: "Zero-inflated models for single-cell RNA-seq data"
author: "The ZINB team"
date: '`r Sys.Date()`'
output: 
    pdf_document:
bibliography: bibli.bib
header-includes:
   - \usepackage{bbm}
   - \usepackage{amsthm}
   - \newtheorem{lemma}{Lemma}
   - \newtheorem{corollary}{Corollary}
---

```{r options, echo=FALSE, results="hide", message=FALSE, error=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.align="center", cache=FALSE, error=FALSE, message=FALSE, echo=TRUE, warning=FALSE, results="markup")
```

## Introduction

Single-cell RNA sequencing (scRNA-Seq) is a powerful and relatively young technique to characterize molecular states of individual cells through their transcriptional profiles. It represents a major advance with respect to standard bulk RNA sequencing, which is only capable of measuring gene expression levels averaged over millions of cells. Such averaged gene expression profiles may describe and characterize the global state of a tissue but cannot afford to study its heterogeneity and completely mask signals coming from individual cells. Accessing cell-to-cell variability is crucial for understanding many important biological processes such as tissue development and cancer. To be continued...

## Model

For any $\mu\geq 0$ and $\theta>0$, let $f_{NB}( \cdot \,;\mu,\theta)$ denote the probability mass function (p.m.f.) of the negative binomial (NB) distribution with mean $\mu$ and inverse dispersion parameter $\theta$, namely:
$$
\forall y\in\mathbb{N},\quad f_{NB}(y;\mu,\theta) = \frac{\Gamma(y+\theta)}{\Gamma(y+1)\Gamma(\theta)} \left(\frac{\theta}{\theta+\mu}\right)^{\theta} \left(\frac{\mu}{\mu + \theta}\right)^y \,.
$$
Note that another parametrization of the NB p.m.f. is in terms of the dispersion parameter $\phi=\theta^{-1}$ (although $\theta$ is also sometimes called dispersion parameter in the literature). In both cases, the mean of the NB distribution is $\mu$ and its variance is:
$$
\sigma^2 = \mu + \frac{\mu^2}{\theta}= \mu + \phi\mu^2 \,.
$$
In particular, the NB distribution boils down to a Poisson distribution when $\phi=0 \Leftrightarrow \theta = +\infty$.

For any $\pi\in[0,1]$, let $f_{ZINB}( \cdot \,;\mu,\theta, \pi)$ be the p.m.f. of the zero-inflated negative binomial (ZINB) distribution given by:
$$
\forall y\in\mathbb{N},\quad f_{ZINB}(y;\mu,\theta, \pi) = \pi \delta_0(y) + (1-\pi) f_{NB}(y;\mu,\theta) \,,
$$
where $\delta_0(\cdot)$ is the Dirac function. Here, $\pi$ can be interpreted as the probability that a $0$ is observed instead of the actual count, resulting in an inflation of zeros compared to the NB distribution, hence the name ZINB.

Given $n$ samples (typically, $n$ single cells) and $J$ features (typically, $J$ genes) that can be counted in each samples, let $Y_{ij}$ be the count of feature $j$ (for $j=1,\ldots,J$) in sample $i$ ($i=1,\ldots,n$). To account for various technical and biological effects frequent in particular in single-cell sequencing technologies, we model $Y_{ij}$ as a random variable following a ZINB distribution with parameters $\mu_{ij}, \theta_{ij}$, and $\pi_{ij}$, and consider the following model for the parameters:
\begin{align}
\label{eq:model1}
\ln(\mu_{ij}) &= \left( X\beta_\mu + (V\gamma_\mu)^\top + W\alpha_\mu + O_\mu\right)_{ij}\,,\\
\label{eq:model2}
\text{logit}(\pi_{ij}) &= \left(X\beta_\pi + (V\gamma_\pi)^\top + W\alpha_\pi + O_\pi\right)_{ij} \,,\\
\label{eq:model3}
\ln(\theta_{ij}) &= \zeta_j \,,
\end{align}
where
$$
\text{logit}(\pi) = \ln\left(\frac{\pi}{1-\pi}\right)\,
$$
and:
\begin{itemize}
\item $X$ is a known $n \times M$ design matrix corresponding to $M$ cell-level covariates and ${\bf \beta}=(\beta_\mu,\beta_\pi)$ its associated $M \times J$ matrices of regression parameters. $X$ can typically include covariates that induce variation of interest, such as cell types, or covariates that induce unwanted variation, such as batch or QC measures. It can also include a constant column ${\bf 1}_n$ to account for gene-specific intercepts.
\item $V$ is a known $J \times L$ matrix corresponding to $J$ gene-level covariates, such as gene length or GC-content, and ${\bf \gamma} = (\gamma_\mu , \gamma_\pi)$ its associated $L\times n$ matrices of regression parameters. $V$ can also include a constant column ${\bf 1}_J$ to account for cell-specific intercepts, such as size factors representing differences in library sizes.
\item $W$ is an unobserved $n \times K$ matrix corresponding to $K$ unknown cell-level covariates, which could be of “unwanted variation” as in RUV or of interest (such as cell type), and ${\bf \alpha} = (\alpha_\mu,\alpha_{\pi})$ its associated $K \times J$ matrices of regression parameters.
\item $O_\mu$ and $O_\pi$ are known $n \times J$ matrices of offsets.
\item $\zeta\in\mathbb{R}^J$ is a vector of gene-specific dispersion parameters on the log scale.
\end{itemize}

This model deserves a few comments.
\begin{itemize}
\item The model extends the RUV framework to the ZINB distribution (thus far, RUV had only been implement for linear and log-linear regression). It differs in interpretation from RUV in the $W\alpha$ factor, which is not necessarily considered unwanted; this term generally corresponds to unknown low-dimensional variation, that could be due to unwanted techical effects (as in RUV), such as batch effects, or to biological effects which could be of interest, such as cell cycle or cell differentiation, as typically assumed in other factor models such as principal component analysis (PCA) or independent component analysis (ICA).
\item By default, $X$ and $V$ contain a constant column of ones, to account, respectively, for gene-specific (e.g., mean expression level) and cell-specific (e.g., size factors) variation. In that case, $X$ and $V$ are of the form $X=[{\bf 1}_n , X^0]$ and $V = [{\bf 1}_J , V^0]$, and we can similarly decompose the corresponding parameters as $\beta = [\beta^1 ; \beta^0]$ and $\gamma = [\gamma^1 ; \gamma^0]$, where $\beta^1 \in \mathbb{R}^{1\times J}$ is a vector of gene-specific intercepts and $\gamma^1 \in \mathbb{R}^{1\times n}$ a vector of cell-specific intercepts. The representation ${\bf 1}_n \beta^1 + ({\bf 1}_J \gamma^1)^\top$ is then not unique, but could be made unique by adding a constant and constraining $\beta^1$ and $\gamma^1$ to each have elements summing to zero.
\item The $X$ and $V$ matrices could differ in the modelling of $\mu$ and $\pi$ if we assume that some known factors do not affect both $\mu$ and $\pi$; to keep notation simple and consistent we use the same matrices, but will implicitly assume that some parameters may be constrained to be $0$ if needed.
\item When $X={\bf 1}_n$ and $V={\bf 1}_J$, the model is a factor model akin to PCA, where $W$ is a factor matrix and $(\alpha_\mu, \alpha_\pi)$ are loading matrices.
\item By allowing the parameters to differ between the models of $\mu$ and $\pi$, we can model and test for differences in NB mean or in ZI probability.
\item We limit ourselves to a gene-dependent dispersion parameter. More complicated models for $\theta_{ij}$ could be investigated, such as a model similar to $\mu_{ij}$ or a functional of the form $\theta_{ij} = f(\mu_{ij})$, but we restrict ourselves to a simpler model that has been shown to be largely sufficient in bulk RNA-Seq analysis.
\end{itemize}

## Parameter estimation
The input to the model are the matrices $X$, $V$, $O_\mu$, and $O_\pi$ and the integer $K$; the parameters to be inferred are $\beta=(\beta_\mu,\beta_\pi)$, $\gamma=(\gamma_\mu,\gamma_\pi)$, $W$, $\alpha=(\alpha_\mu,\alpha_\pi)$, and $\zeta$. Given an $n\times J$ matrix of counts $Y$, the log-likelihood function is
$$
\ell(\beta,\gamma,W,\alpha,\zeta) = \sum_{i=1}^n \sum_{j=1}^J \ln f_{ZINB}(Y_{ij};\mu_{ij},\theta_{ij}, \pi_{ij}) \,,
$$
where $\mu_{ij}$, $\theta_{ij}$, and $\pi_{ij}$ depend on $(\beta,\gamma,W,\alpha,\zeta)$ through \eqref{eq:model1}-\eqref{eq:model3}. 

To infer the parameters, we follow a penalized maximum likelihood approach, by trying to solve
$$
\max_{\beta,\gamma,W,\alpha,\zeta} \left\{\ell(\beta,\gamma,W,\alpha,\zeta) - \text{Pen}(\beta,\gamma,W,\alpha,\zeta) \right\}\,,
$$
where $\text{Pen}(\cdot)$ is a regularization term to reduce overfitting and improve the numerical stability of the optimization problem in the setting of many parameters. For a set of nonnegative regularization parameters $\left( \epsilon_\beta , \epsilon_\gamma, \epsilon_W, \epsilon_\alpha, \epsilon_\zeta \right)$, we set
$$
\text{Pen}(\beta,\gamma,W,\alpha,\zeta) = \frac{\epsilon_{\beta}}{2} \|\beta^0\|^2 + \frac{\epsilon_{\gamma}}{2} \|\gamma^0\|^2 + \frac{\epsilon_{W}}{2}\|W\|^2 + \frac{\epsilon_{\alpha}}{2}\|\alpha\|^2 + \frac{\epsilon_{\zeta}}{2}\text{Var}(\zeta) \,,
$$
where $\beta^0$ and $\gamma^0$ denote the matrices $\beta$ and $\gamma$ without the rows corresponding to the intercepts if an unpenalized intercept is included in the model, $\|\cdot\|$ is the Frobenius matrix norm, and $\text{Var}(\zeta) = 1/(J-1) \sum_{i=1}^J \left(\zeta_i - (\sum_{j=1}^J \zeta_j)/J\right)^2$ is the variance of the elements of $\zeta$ (using the unbiased sample variance statistic). The penalty tends to shrink the estimated parameters to $0$, except for the cell- and gene-specific intercepts which are not penalized and the dispersion parameters which are not shrunk towards $0$ but instead towards a constant value across genes. Note also that the likelihood only depends on $W$ and $\alpha$ through their product $R=W\alpha$ and that the penalty ensures that at the optimum $W$ and $\alpha$ have the structure described in the following result which generalizes standard results such as [@Srebro2005Maximum, Lemma 1; @Mazumder2010Spectral, Lemma 6].
\begin{lemma}\label{lem:tracenorm}
For any matrix $R$ and positive scalars $s$ and $t$, the following holds:
$$
\min_{S,T\,:\,R=ST} \frac{1}{2} \left( s\|S\|^2 + t\|T\|^2\right) = \sqrt{st} \|R\|_* \,.
$$
If $R=R_L R_\Sigma R_R$ is a SVD decomposition of $R$, then a solution to this optimization problem is:
$$
S = \left(\frac{t}{s}\right)^{\frac{1}{4}} R_L R_\Sigma^{\frac{1}{2}} \,,\quad T = \left(\frac{s}{t}\right)^{\frac{1}{4}} R_\Sigma^{\frac{1}{2}} R_R \,.
$$
\end{lemma}
\begin{proof}
Let $\tilde{S} = \sqrt{s}S$, $\tilde{T} = \sqrt{t}T$, and $\tilde{R} = \sqrt{st}R$. Then, $\|\tilde{S}\|^2 = s\|S\|^2$, $\|\tilde{T}\|^2 = t\|T\|^2$, and $\tilde{S}\tilde{T} = \sqrt{st}ST$, so that the optimization problem is equivalent to:
$$
\min_{\tilde{S}, \tilde{T} \,:\,\tilde{S}\tilde{T}=\tilde{R}} \frac{1}{2} \left( \|\tilde{S}\|^2 + \|\tilde{T}\|^2\right) \,,
$$
which by [@Mazumder2010Spectral, Lemma 6] has optimum value $\|\tilde{R}\|_* = \sqrt{st}\|R\|_*$ reached at $\tilde{S} = \tilde{R}_L \tilde{R}_\Sigma^{\frac{1}{2}}$ and $\tilde{T} =  \tilde{R}_\Sigma^{\frac{1}{2}} \tilde{R_R}$, where $\tilde{R}_R \tilde{R}_\Sigma \tilde{R}_R$ is a SVD decomposition of $\tilde{R}$. Observing that $\tilde{R}_L=R_L$,  $\tilde{R}_R=R_R$, and $\tilde{R}_\Sigma=\sqrt{st}\tilde{R}_\Sigma$ gives that a solution of the optimization problem is $S=s^{-1/2} \tilde{S} = s^{-1/2} R_L (st)^{1/4} R_\Sigma^{1/2} = (t/s)^{1/4} R_L R_\Sigma^{1/2}$. A similar argument for $T$ concludes the proof.
\end{proof}
This lemma implies in particular that at any local maximum of the penalized log-likelihood, $W$ and $\alpha^\top$ have orthogonal columns, which is useful for visualization or interpretation of latent factors.

To balance the penalties applied to the different matrices in spite of their different sizes, a natural choice is to fix $\epsilon>0$ and set
$$
\epsilon_\beta = \frac{\epsilon}{J}\,,\quad
\epsilon_\gamma = \frac{\epsilon}{n}\,,\quad
\epsilon_W = \frac{\epsilon}{n}\,,\quad
\epsilon_\alpha = \frac{\epsilon}{J}\,,\quad
\epsilon_\zeta = \epsilon\,.
$$
In particular, from Lemma \ref{lem:tracenorm}, we easily deduce the following characterization of the penalty on $W$ and $\alpha$, which shows that the entries in the matrices $W$ and $\alpha$ have similar standard deviation after optimization:
\begin{corollary}\label{coro:tracenorm}
For any $n\times J$ matrix $R$ and positive scalars $\epsilon$, the following holds. 
$$
\min_{W,\alpha\,:\,R=W\alpha} \frac{\epsilon}{2} \left( \frac{1}{n}\|W\|^2 + \frac{1}{J}\|\alpha\|^2\right) = \frac{\epsilon}{\sqrt{nJ}} \|R\|_* \,.
$$
If $R=R_L R_\Sigma R_R$ is a SVD decomposition of $R$, then a solution to this optimization problem is:
$$
W = \left(\frac{n}{J}\right)^{\frac{1}{4}} R_L R_\Sigma^{\frac{1}{2}} \,,\quad T = \left(\frac{J}{n}\right)^{\frac{1}{4}} R_\Sigma^{\frac{1}{2}} R_R \,.
$$
In particular, for any $i=1,\ldots,\min(n,J)$,
$$
\frac{1}{n} \sum_{j=1}^n S_{j,i}^2 = \frac{1}{J} \sum_{j=1}^J T_{i,j}^2 = \frac{[R_\Sigma]_{i,i}}{\sqrt{nJ}}.
$$
\end{corollary}


The penalized likelihood is however not concave, making its maximization computationally challenging. We instead find a local maximum, starting from a smart initialization and iterating a numerical optimization scheme until local convergence, as described below.

### Initialization
To initialize the set of parameters we approximate the count distribution by a log-normal distribution and explicitly separate zero and non-zero values, as follows:
\begin{enumerate}
\item Set $\mathcal{P} = \left\{(i,j)\,:\,Y_{ij}>0\right\}$.
\item Set $L_{ij} = \ln(Y_{ij}) - (O_\mu)_{ij}$ for all $(i,j)\in\mathcal{P}$. 
\item Set $\hat{Z}_{ij} = 1$ if $(i,j)\in\mathcal{P}$, $\hat{Z}_{ij} =0$ otherwise.
\item Estimate $\beta_\mu$ and $\gamma_\mu$ by solving the convex ridge regression problem:
$$
\min_{\beta_\mu, \gamma_\mu} \sum_{(i,j)\in\mathcal{P}} \left(L_{ij} - (X\beta_{\mu})_{ij} -  (V\gamma_\mu)_{ji} \right)^2 +  \frac{\epsilon_\beta}{2} \|\beta_\mu^0\|^2 + \frac{\epsilon_\gamma}{2} \|\gamma_\mu^0\|^2 \,.
$$
This is a standard ridge regression problem, but with a potentially huge design matrix, with up to $nJ$ rows and $MJ+nL$ columns. To solve it efficiently, we alternate the estimation of $\beta_\mu$ and $\gamma_\mu$. Specifically, we initialize parameter values as:
$$
\hat{\beta}_\mu \leftarrow 0 \,,\quad\hat{\gamma}_\mu \leftarrow 0\, 
$$
and repeat the following two steps a few times (or until convergence):
\begin{enumerate}
\item Optimization in $\gamma_\mu$, which can be performed independently and in parallel for each cell:
$$
\hat{\gamma}_\mu \in \arg\min_{\gamma_\mu} \sum_{(i,j)\in\mathcal{P}} \left(L_{ij} - (X\hat{\beta}_{\mu})_{ij} - (V\gamma_\mu)_{ji} \right)^2 + \frac{\epsilon_\gamma}{2} \|\gamma_\mu^0\|^2\,.
$$
\item Optimization in $\beta_\mu$, which can be performed independently and in parallel for each gene:
$$
\hat{\beta}_\mu \in \arg\min_{\beta_\mu} \sum_{(i,j)\in\mathcal{P}} \left(L_{ij} - (V\hat{\gamma}_\mu)_{ji} - (X\beta_{\mu})_{ij} \right)^2 + \frac{\epsilon_\beta}{2} \|\beta_\mu^0\|^2\,.
$$
\end{enumerate}
\item Estimate $W$ and $\alpha_\mu$ by solving
$$
\left(\hat{W},\hat{\alpha}_\mu\right) \in \arg\min_{W,\alpha_\mu} \sum_{(i,j)\in\mathcal{P}} \left(L_{ij} - (X\hat{\beta}_{\mu})_{ij} - (V\hat{\gamma}_\mu)_{ji} - (W\alpha_\mu)_{ij} \right)^2 + \frac{\epsilon_W}{2} \|W\|^2 + \frac{\epsilon_\alpha}{2} \|\alpha_\mu\|^2\,.
$$
Denoting by $D=L-X\hat{\beta}-(V\hat{\gamma})^\top$, this problem can be rewritten as:
$$
\min_{W,\alpha} \|D-W\alpha\|^2_{\mathcal{P}} + \frac{1}{2} \left(\epsilon_W \|W\|^2 + \epsilon_\alpha\|\alpha\|^2\right)\,,
$$
where $\|A\|^2_\mathcal{P} = \sum_{(i,j)\in\mathcal{P}}A_{ij}^2$. By Lemma~\ref{lem:tracenorm}, if $K$ is large enough, one can first solve the convex optimization problem:
\begin{equation}\label{eq:softimpute}
\hat{R} \in \arg\min_{R\,:\,\text{rank}(R)\leq K} \|D - R\|^2_{\mathcal{P}} + \sqrt{\epsilon_W \epsilon_\alpha} \|R\|_*\,
\end{equation}
and set 
$$
W = \left(\frac{\epsilon_\alpha}{\epsilon_W}\right)^{\frac{1}{4}} R_L R_\Sigma^{\frac{1}{2}} \,,\quad \alpha = \left(\frac{\epsilon_W}{\epsilon_\alpha}\right)^{\frac{1}{4}} R_\Sigma^{\frac{1}{2}} R_R \,,
$$
where $\hat{R} = R_L R_\Sigma R_R$ is the SVD of $\hat{R}$. This solution is exact when $K$ is at least equal to the rank of the solution of the unconstrained problem (\ref{eq:softimpute}), which we solve with the \texttt{softImpute::softImpute()} function
[@Mazumder2010Spectral]. If $K$ is smaller, then (\ref{eq:softimpute}) becomes a non-convex optimization problem whose global optimum may be challenging to find. In that case we also use the rank-constrained version of \texttt{softImpute::softImpute()} to obtain a good local optimum.
\item Estimate $\beta_\pi$, $\gamma_\pi$, and $\alpha_\pi$ by solving the regularized logistic regression problem:
\begin{multline}
\min_{\left(\beta_\pi,\gamma_\pi, \alpha_\pi\right)} \sum_{(i,j)}
\Big[-\hat{Z}_{ij} (X\beta_\pi + (V\gamma_\pi)^\top + \hat{W}\alpha_\pi)_{ij} \\
+ \ln\left( 1 + e^{(X\beta_\pi + (V\gamma_\pi)^\top + \hat{W}\alpha_\pi)_{ij}}\right)  \Big] + \frac{\epsilon_\beta}{2}\|\beta_\pi\|^2 + \frac{\epsilon_\gamma}{2}\|\gamma_\pi\|^2 + \frac{\epsilon_\alpha}{2}\|\alpha_\pi\|^2 \,.
\end{multline}
This is a standard ridge logistic regression problem, but with a potentially huge design matrix, with up to $nJ$ rows and $MJ+nL$ columns. To solve it efficiently, we alternate the estimation of $\beta_\pi$, $\gamma_\pi$, and $\alpha_\pi$. Specifically, we initialize parameter values as:
$$
\hat{\beta}_\pi \leftarrow 0 \,,\quad\hat{\gamma}_\pi \leftarrow 0\,,\quad\hat{\alpha}_\pi\leftarrow 0\,
$$
and repeat the following two steps a few times (or until convergence):
\begin{enumerate}
\item Optimization in $\gamma_\pi$:
\begin{multline}
\hat{\gamma}_\pi \in \arg\min_{\gamma_\pi} \sum_{(i,j)}
\Big[-\hat{Z}_{ij} (X\hat{\beta}_\pi + (V\gamma_\pi)^\top + \hat{W}\hat{\alpha}_\pi)_{ij} \\
+ \ln\left( 1 + e^{(X\hat{\beta}_\pi + (V\gamma_\pi)^\top + \hat{W}\hat{\alpha}_\pi)_{ij}}\right)  \Big] + \frac{\epsilon_\gamma}{2}\|\gamma_\pi\|^2 \,.
\end{multline}
Note that this problem can be solved for each cell ($i$) independently and in parallel. When there is no gene covariate besides the constant intercept, the problem is easily solved by setting $(\hat{\gamma}_\pi)_i$ to the logit of the proportion of zeros in each cell.
\item Optimization in $\beta_\pi$ and $\alpha_\pi$:
\begin{multline}
\left(\hat{\beta}_\pi, \hat{\alpha}_\pi\right) \in \arg\min_{\left(\beta_\pi,\alpha_\pi\right)} \sum_{(i,j)}
\Big[-\hat{Z}_{ij} (X\beta_\pi + (V\hat{\gamma}_\pi)^\top + \hat{W}\alpha_\pi)_{ij} \\
+ \ln\left( 1 + e^{(X\beta_\pi + (V\hat{\gamma}_\pi)^\top + \hat{W}\alpha_\pi)_{ij}}\right)  \Big] + \frac{\epsilon_\beta}{2}\|\beta_\pi\|^2 + \frac{\epsilon_\alpha}{2}\|\alpha_\pi\|^2 \,.
\end{multline}
\end{enumerate}
\item Initialize $\hat{\zeta}=0$.
\end{enumerate}

### Optimization
After initialization, we maximize locally the penalized log-likelihood by alternating optimization over the dispersion parameters and left- and right-factors, iterating the following steps until convergence:
\begin{enumerate}
\item Dispersion optimization:
$$
\hat{\zeta} \leftarrow \arg\max_{\zeta} \left\{\ell(\hat{\beta}, \hat{\gamma}, \hat{W}, \hat{\alpha}, \zeta) - \frac{\epsilon_\zeta}{2} \text{Var}(\zeta) \right\} \,.
$$
To solve this problem, we start by estimating a common dispersion parameter for all the genes, by maximizing the objective function under the constraint that $\text{Var}(\zeta)=0$; in practice, we use a derivative-free one-dimensional optimization vector over the range $[-50,50]$. We then optimize the objective function by a quasi-Newton optimization scheme starting from the constant solution found by the first step. To derive the gradient of the objective function used by the optimization procedure, note that the derivative of the NB log-density is:
$$
\frac{\partial}{\partial \theta} \ln f_{NB}(y;\mu,\theta) = \Psi(y+\theta) - \Psi(\theta) + \ln\theta + 1 - \ln(\mu+\theta) - \frac{y+\theta}{\mu + \theta} \,,
$$
where $\Psi(z) = \Gamma'(z)/\Gamma(z)$ is the digamma function. We therefore get the derivative of the ZINB density as follows, for any $\pi\in[0,1]$:
\begin{itemize}
\item If $y>0$, $f_{ZINB}(y;\mu,\theta,\pi) = (1-\pi)f_{NB}(y;\mu,\theta)$ therefore
$$
\frac{\partial}{\partial \theta} \ln f_{ZINB}(y;\mu,\theta) = \Psi(y+\theta) - \Psi(\theta) + \ln\theta + 1 - \ln(\mu+\theta) - \frac{y+\theta}{\mu + \theta} \,.
$$
\item For $y=0$, $\frac{\partial}{\partial \theta} \ln f_{NB}(0;\mu,\theta) = \ln\theta + 1 - \ln(\mu+\theta) - \frac{\theta}{\mu + \theta}$, therefore
$$
\frac{\partial}{\partial \theta} \ln f_{ZINB}(y;\mu,\theta) =
\frac{\ln\theta + 1 - \ln(\mu+\theta) - \frac{\theta}{\mu + \theta}}{1 + \frac{\pi(\mu + \theta)^\theta}{(1-\pi) \theta^\theta}}.
$$\end{itemize}
The derivative of the objective function w.r.t. $\zeta_j$, for $j=1,\ldots,J$, is then easily obtained by
$$
\sum_{i=1}^n \theta_j \frac{\partial}{\partial \theta} \ln f_{ZINB}(y_{ij};\mu_{ij},\theta_j) - \frac{\epsilon_\zeta}{J-1}\left(\zeta_j - \frac{1}{J}\sum_{k=1}^J \zeta_k\right)\,.
$$
(Note that the $J-1$ term in the denominator comes from the use of the unbiased sample variance statistic in the penalty for $\zeta$.)
\item Left-factor (cell-specific) optimization:
\begin{equation}\label{eq:leftoptim}
\left(\hat{\gamma}, \hat{W} \right) \leftarrow \arg\max_{\left(\gamma, W\right)} \left\{\ell(\hat{\beta}, \gamma, W, \hat{\alpha}, \hat{\zeta}) - \frac{\epsilon_\gamma}{2} \|\gamma^0\|^2 - \frac{\epsilon_W}{2} \|W\|^2 \right\}\,.
\end{equation}
Note that this optimization can be performed independently and in parallel for each cell $i=1,\ldots,n$. For this purpose, we consider a subroutine \texttt{solveZinbRegression}$(y, A_\mu, B_\mu, C_\mu, A_\pi, B_\pi, C_\pi, C_\theta)$ to find a set of vectors $(a_\mu, a_\pi, b)$ that locally maximize the log-likelihood of a ZINB model for a vector of counts $y$ parametrized as follows:
\begin{align*}
\ln(\mu) &= A_\mu a_\mu + B_\mu b + C_\mu\,,\\
\text{logit}(\pi) &= A_\pi a_\pi + B_\pi b + C_\pi \,,\\
\ln(\theta) &= C_\theta \,.
\end{align*}
To solve (\ref{eq:leftoptim}) for cell $i$ we call \texttt{solveZinbRegression} with the following parameters:
$$
\begin{cases}
a_\mu &= \gamma_\mu[.,i]\\
a_\pi &= \gamma_\pi[.,i] \\
b &= W[i,.]^\top \\
y & = Y[i,]^\top \\
A_\mu &= V_\mu \\
B_\mu &= \alpha_\mu^\top \\
C_\mu &= (X_\mu[i,.] \beta_\mu + O_\mu[i,.])^\top\\
A_\pi &= V_\pi \\
B_\pi &= \alpha_\pi^\top \\
C_\pi &= (X_\pi[i,.] \beta_\pi + O_\pi[i,.])^\top\\
C_\theta &= \zeta
\end{cases}.
$$
\item Right-factor (gene-specific) optimization:
$$
\left(\hat{\beta}, \hat{\alpha} \right) \leftarrow \arg\max_{\left(\beta, \alpha \right)} \left\{\ell(\beta, \hat{\gamma},\hat{W}, \alpha, \hat{\zeta}) - \frac{\epsilon_\beta}{2} \|\beta^0\|^2 - \frac{\epsilon_\alpha}{2} \|\alpha\|^2\right\}\,.
$$
Note that this optimization can be performed independently and in parallel for each gene $j=1,\ldots,J$, by calling \texttt{solveZinbRegression} with the following paramters:
$$
\begin{cases}
a_\mu &= (\beta_\mu[.,j] ; \alpha_\mu[.,j]) \\
a_\pi &= (\beta_\pi[.,j] ; \alpha_\pi[.,j]) \\
b &= \emptyset \\
y & = Y[.,j] \\
A_\mu &= [X_\mu , W] \\
B_\mu &= \emptyset \\
C_\mu &= (V_\mu[j,.] \gamma_\mu)^\top + O_\mu[.,j]\\
A_\pi &= [X_\pi , W] \\
B_\pi &= \emptyset \\
C_\pi &= (V_\pi[j,.] \gamma_\pi)^\top + O_\pi[.,j]\\
C_\theta &= \zeta_j {\bf 1}_n
\end{cases}.
$$
\item Orthogonalization: 
$$
\left(\hat{W}, \hat{\alpha} \right) \leftarrow \arg\min_{(W,\alpha)\,:\,W\alpha = \hat{W}\hat{\alpha}} \frac{1}{2} \left( \epsilon_W \|W\|^2 + \epsilon_\alpha \|\alpha\|^2 \right)\,.
$$
This is obtained by applying Lemma~\ref{lem:tracenorm}, starting from an SVD decomposition of the current $\hat{W}\hat{\alpha}$. Note that this step not only allows to maximize locally the penalized log-likelihood, but also ensures that the columns of $W$ stay orthogonal to each other during optimization.
\end{enumerate}

**TODO: write gradients and explain how steps 1-3 are implemented using a single function**

# References
